{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RePath Model Retraining Workflow\n",
    "\n",
    "This notebook walks through benchmark prep, annotation seeding, candidate training/export, and candidate evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Paths and Environment\n",
    "By default this notebook resolves roots in this order: env override -> local `repath-model` path -> fallback `repath-mobile` path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def resolve_repo_root(start: Path) -> Path:\n",
    "    start = Path(start).resolve()\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / \"pyproject.toml\").exists() and (candidate / \"scripts\").is_dir():\n",
    "            return candidate\n",
    "    return start\n",
    "\n",
    "\n",
    "def resolve_root(env_key: str, candidates):\n",
    "    value = os.environ.get(env_key, \"\").strip()\n",
    "    if value:\n",
    "        return Path(value).expanduser().resolve()\n",
    "    for candidate in candidates:\n",
    "        candidate = candidate.resolve()\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    return candidates[0].resolve()\n",
    "\n",
    "\n",
    "ROOT = resolve_repo_root(Path.cwd())\n",
    "os.chdir(ROOT)\n",
    "MODEL_ASSETS_ROOT = resolve_root(\n",
    "    \"REPATH_MODEL_ASSETS_ROOT\",\n",
    "    [ROOT / \"assets/models\", ROOT / \"../repath-mobile/assets/models\"],\n",
    ")\n",
    "BENCHMARK_ROOT = resolve_root(\n",
    "    \"REPATH_BENCHMARK_ROOT\",\n",
    "    [ROOT / \"test/benchmarks\", ROOT / \"../repath-mobile/test/benchmarks\"],\n",
    ")\n",
    "TRAINING_ARTIFACTS_ROOT = resolve_root(\n",
    "    \"REPATH_TRAINING_ARTIFACTS_ROOT\",\n",
    "    [ROOT / \"artifacts\", ROOT / \"../repath-mobile/ml/artifacts\"],\n",
    ")\n",
    "KAGGLE_DIR = os.environ.get(\"KAGGLE_WASTE_DIR\", \"\")\n",
    "\n",
    "print({\n",
    "    \"repath_model\": str(ROOT),\n",
    "    \"model_assets_root\": str(MODEL_ASSETS_ROOT),\n",
    "    \"benchmark_root\": str(BENCHMARK_ROOT),\n",
    "    \"training_artifacts_root\": str(TRAINING_ARTIFACTS_ROOT),\n",
    "    \"kaggle_dir\": KAGGLE_DIR,\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Benchmark Prep and Coverage\n",
    "Run sync, holdout build, resolved manifest build, coverage, and audit checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([\n",
    "    \"python3\", \"scripts/evaluation/sync_benchmark_progress.py\",\n",
    "    \"--manifest\", str(BENCHMARK_ROOT / \"municipal-benchmark-manifest-v2.json\"),\n",
    "    \"--completed\", str(BENCHMARK_ROOT / \"benchmark-labeled.csv\"),\n",
    "], check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    \"python3\", \"scripts/evaluation/build_supported_holdout_manifest.py\",\n",
    "    \"--labels\", str(MODEL_ASSETS_ROOT / \"yolo-repath.labels.json\"),\n",
    "    \"--input-csv\", str(BENCHMARK_ROOT / \"benchmark-labeled.csv\"),\n",
    "    \"--retraining-manifest\", str(TRAINING_ARTIFACTS_ROOT / \"retraining/retraining-manifest.json\"),\n",
    "    \"--cache-dir\", str(BENCHMARK_ROOT / \"images/supported-holdout\"),\n",
    "    \"--out\", str(BENCHMARK_ROOT / \"benchmark-manifest.supported-holdout.json\"),\n",
    "]\n",
    "if KAGGLE_DIR:\n",
    "    cmd.extend([\"--kaggle-dir\", KAGGLE_DIR])\n",
    "subprocess.run(cmd, check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([\n",
    "    \"python3\", \"scripts/evaluation/build_resolved_benchmark_manifest.py\",\n",
    "    \"--manifest\", str(BENCHMARK_ROOT / \"municipal-benchmark-manifest-v2.json\"),\n",
    "    \"--completed\", str(BENCHMARK_ROOT / \"benchmark-labeled.csv\"),\n",
    "    \"--cache-dir\", str(BENCHMARK_ROOT / \"images\"),\n",
    "    \"--out\", str(BENCHMARK_ROOT / \"municipal-benchmark-manifest.resolved.json\"),\n",
    "], check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([\n",
    "    \"python3\", \"scripts/evaluation/check_benchmark_coverage.py\",\n",
    "    \"--taxonomy\", str(MODEL_ASSETS_ROOT / \"municipal-taxonomy-v1.json\"),\n",
    "    \"--manifest\", str(BENCHMARK_ROOT / \"municipal-benchmark-manifest.resolved.json\"),\n",
    "    \"--out\", str(BENCHMARK_ROOT / \"benchmark-coverage-report.resolved.json\"),\n",
    "], check=True)\n",
    "\n",
    "subprocess.run([\n",
    "    \"python3\", \"scripts/evaluation/audit_benchmark_dataset.py\",\n",
    "    \"--manifest\", str(BENCHMARK_ROOT / \"municipal-benchmark-manifest.resolved.json\"),\n",
    "    \"--taxonomy\", str(MODEL_ASSETS_ROOT / \"municipal-taxonomy-v1.json\"),\n",
    "    \"--out\", str(BENCHMARK_ROOT / \"benchmark-dataset-audit.resolved.json\"),\n",
    "], check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Annotation + Training\n",
    "These commands expect annotation bundle content under `TRAINING_ARTIFACTS_ROOT / retraining/annotation-bundle`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([\n",
    "    \"python3\", \"scripts/annotation/seed_annotation_boxes.py\",\n",
    "    \"--bundle-root\", str(TRAINING_ARTIFACTS_ROOT / \"retraining/annotation-bundle\"),\n",
    "    \"--model\", str(MODEL_ASSETS_ROOT / \"yolo-repath.tflite\"),\n",
    "    \"--labels\", str(MODEL_ASSETS_ROOT / \"yolo-repath.labels.json\"),\n",
    "], check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional quick dry run\n",
    "subprocess.run([\n",
    "    \"python3\", \"scripts/training/train_detector_from_annotation.py\",\n",
    "    \"--bundle-root\", str(TRAINING_ARTIFACTS_ROOT / \"retraining/annotation-bundle\"),\n",
    "    \"--candidate-root\", str(TRAINING_ARTIFACTS_ROOT / \"models/candidates\"),\n",
    "    \"--project\", str(TRAINING_ARTIFACTS_ROOT / \"training-runs\"),\n",
    "    \"--dry-run\",\n",
    "], check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Candidate Evaluation\n",
    "Run candidate benchmark + analysis + comparison against baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([\n",
    "    \"python3\", \"scripts/evaluation/benchmark_candidate_model.py\",\n",
    "    \"--candidates-root\", str(TRAINING_ARTIFACTS_ROOT / \"models/candidates\"),\n",
    "    \"--manifest\", str(BENCHMARK_ROOT / \"municipal-benchmark-manifest.resolved.json\"),\n",
    "    \"--out\", str(BENCHMARK_ROOT / \"latest-results.candidate.json\"),\n",
    "    \"--supported-only\",\n",
    "], check=True)\n",
    "\n",
    "subprocess.run([\n",
    "    \"python3\", \"scripts/evaluation/compare_benchmark_results.py\",\n",
    "    \"--baseline\", str(BENCHMARK_ROOT / \"latest-results.json\"),\n",
    "    \"--candidate\", str(BENCHMARK_ROOT / \"latest-results.candidate.json\"),\n",
    "    \"--out\", str(BENCHMARK_ROOT / \"latest-results.compare.json\"),\n",
    "], check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What To Look For\n",
    "- `benchmark-coverage-report.resolved.json`: coverage gaps and unknown labels.\n",
    "- `benchmark-dataset-audit.resolved.json`: duplicates, missing URLs, class balance recommendations.\n",
    "- `latest-results.candidate.analysis.json`: missed labels and false-positive priorities.\n",
    "- `latest-results.compare.json`: baseline vs candidate metric deltas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
